{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56c7254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional libraries apart from the stanard ML libraries\n",
    "!pip install pytorch-nlp\n",
    "!pip install torchvision\n",
    "!pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9da517cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6f699",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e00e0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('crafted_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc751b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UserID', 'createdAt', 'CollectedAt', 'NumberOfFollowings', 'NumberOfFollowers', 'NumberOfTweets', 'LengthOfScreenName', 'LengthOfDescriptionInUserProfile', 'userType', 'LinkRatio', 'uniqueLinksRatio', 'AtRatio', 'uniqueATRatio', 'averageWords', 'averageChars', 'averageUppercase', 'averageLowercase', 'averagePunctuation', 'LogNumberOfFollowings', 'LogNumberOfFollowers', 'LogNumberOfTweets', 'ratioFollowings_Followers', 'SeriesOfNumberOfFollowings', 'rate_change', 'rate_change_std', 'mean_followers', 'maxTweetDay', 'averageTweetDay', 'sequence', 'sequenceEntropy', 'SequenceRate_Change', 'RatioReplySequence', 'LastTweet', 'LastReply', 'entropyReplyTweetInterval', 'LinkCompressionLength', 'CompressLength', 'first_30_days', 'averageTweetInterval', 'startEndDiff', 'largestFollowerChange', 'longevity', 'retweetCount', 'uniqueHashtags', 'HashtagCount', 'emojis', 'abbrivations', 'textSim', 'hashtagsEntropy', 'HashtagSim', 'LangCount', 'SentimentPositive', 'SentimentNeutral', 'SentimentNegative', 'SentimentPosSTD', 'SentimentNeuSTD', 'SentimentNegSTD', 'largeFollowerIncreaseNumber', 'longevityToLogFollowingsRatio', 'sequencemomentOne', 'sequencemomentTwo', 'sequencemomentThree', 'sequencemomentFour', 'Log_abs_energy', 'binned_entropy', 'Log_cid_ce', 'SeriesCoef', 'SeriesIntercept', 'SequenceComplexity', 'RollWindow_STD', 'RollWindow_Mean', 'RollWindow_Max', 'RollWindow_Min', 'userTweets', 'cleanTweets', 'Topic', 'nmf_0', 'nmf_1', 'nmf_2', 'nmf_3', 'nmf_4', 'nmf_5', 'nmf_6', 'nmf_7', 'nmf_8', 'nmf_9', 'nmf_10', 'nmf_11', 'nmf_12', 'nmf_13', 'nmf_14', 'nmf_15', 'nmf_16', 'nmf_17', 'nmf_18', 'nmf_19', 'nmf_20', 'nmf_21', 'nmf_22', 'nmf_23', 'nmf_24']\n"
     ]
    }
   ],
   "source": [
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912caf9",
   "metadata": {},
   "source": [
    "The list above contains the features used within this neural network. Initially, there was another dataset containing each user's tweets; however, for easier processing for the word embeddings, the \"cleanTweets\" column includes all tweets by a single user concatenated with each other. This concatenation process occurs for each of the users. Furthermore, the text within \"cleanTweets\" has undergone preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41009632",
   "metadata": {},
   "source": [
    "# Import GloVe Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bf50849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(path):\n",
    "    embeddings_index = dict()\n",
    "    f = open(path, 'r' ,encoding=\"utf8\")\n",
    "\n",
    "    for line in f:\n",
    "        try:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except:\n",
    "            f.__next__()\n",
    "\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def create_embedding_matrix(size_of_vocabulary, pretrained_embedding_dim, tokenizer, embeddings_index):\n",
    "    # Create empty matrix\n",
    "    embedding_weights = torch.Tensor(size_of_vocabulary, pretrained_embedding_dim)\n",
    "    # Add relevant word into matrix\n",
    "    # If not included add a random intialisation based on a normal distribution\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_weights[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_weights[i] = torch.from_numpy(np.random.normal(scale=0.6, size=(pretrained_embedding_dim, )))\n",
    "    return embedding_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c55e567e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2195864 word vectors.\n"
     ]
    }
   ],
   "source": [
    "pretrained_embedding_dim = 300\n",
    "embeddings_index = load_embedding('glove.840B.300d.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c9d83c",
   "metadata": {},
   "source": [
    "# Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d6cc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "# These features are all non-numeric\n",
    "del X['UserID']\n",
    "del X['createdAt']\n",
    "del X['CollectedAt']\n",
    "del X['userTweets']\n",
    "del X['SeriesOfNumberOfFollowings']\n",
    "del X['sequence']\n",
    "y = X['userType']\n",
    "del X['userType']\n",
    "\n",
    "# Split the data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "columns = [col for col in X_train.columns if col != 'cleanTweets']\n",
    "\n",
    "# Min-Max Normalisation\n",
    "X_train[columns]=(X_train[columns]-X_train[columns].min())/(X_train[columns].max()-X_train[columns].min())\n",
    "X_test[columns]=(X_test[columns]-X_test[columns].min())/(X_test[columns].max()-X_test[columns].min())\n",
    "\n",
    "X2_train = X_train[columns].values\n",
    "X2_test = X_test[columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab26aa",
   "metadata": {},
   "source": [
    "The above code has ensured that all the numerical features within the dataset have been normalised for both the training and testing set. They were then set as \"X2_train/test\" as this would be the second input for the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c344c3e",
   "metadata": {},
   "source": [
    "# Text Tokenization & Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0beb0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_size(df):\n",
    "    # Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    # Preparing vocabulary on whole dataset\n",
    "    tokenizer.fit_on_texts(df['cleanTweets'].tolist())\n",
    "    return len(tokenizer.word_index) + 1\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "\n",
    "# Preparing vocabulary\n",
    "tokenizer.fit_on_texts(X_train['cleanTweets'].tolist())\n",
    "\n",
    "# Convert Text to integer sequences\n",
    "X1_train = tokenizer.texts_to_sequences(X_train['cleanTweets'].tolist()) \n",
    "X1_test  = tokenizer.texts_to_sequences(X_test['cleanTweets'].tolist())\n",
    "\n",
    "# Max length of padding\n",
    "maxlen = 150\n",
    "\n",
    "# Padding to ensure all entries are the same length\n",
    "X1_train  = torch.Tensor(pad_sequences(X1_train, maxlen=maxlen, padding='post'))\n",
    "X1_test = torch.Tensor(pad_sequences(X1_test, maxlen=maxlen, padding='post'))\n",
    "\n",
    "# Get word vocabulary size\n",
    "size_of_vocabulary = vocab_size(df)\n",
    "\n",
    "# Create word embedding\n",
    "embedding_weights = create_embedding_matrix(size_of_vocabulary, pretrained_embedding_dim, tokenizer, embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e4f3d3",
   "metadata": {},
   "source": [
    "The above code creates a vocabulary list where each word is assigned a number. Each text is then converted to an integer sequence. To ensure all text sequences are the same length, they are padded to a set length of 150. This then creates the \"X1_train/test\" variables, which is the first input for the neural network. It then creates the embedding weights based on the text in dataset using the GloVe pretrained word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a8ac0",
   "metadata": {},
   "source": [
    "# Creating Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd2fe7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Two_Input_Net(nn.Module):\n",
    "    def __init__(self,hidden_size,lin_size, embedding_matrix=embedding_weights):\n",
    "        super(Two_Input_Net, self).__init__()\n",
    "\n",
    "        # Layer 1a: Embedding Layer (Input)\n",
    "        self.embedding = nn.Embedding(size_of_vocabulary, pretrained_embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        # Layer 1b: Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(pretrained_embedding_dim, maxlen, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Layer 1c: MaxPooling, BatchNormalisation and Dropout\n",
    "        self.globalMaxPool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.batchnorm = nn.BatchNorm1d(maxlen)\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        \n",
    "        # Layer 2a: Dense Layer (Input)\n",
    "        self.linear_one = nn.Linear(X2_train.shape[1], 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Layer 2b: Dense Layer\n",
    "        self.linear_two = nn.Linear(10, lin_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Layer 3a: Concatation Section which is described in forward().\n",
    "        \n",
    "        # Layer 3b: Dense Layer\n",
    "        self.linear = nn.Linear(300, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Layer 4: Output Layer return probability value from 0 to 1.\n",
    "        self.out = nn.Linear(10, 1)\n",
    "        self.outFunc = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x[0] is the text feautes\n",
    "        # x[1] is the numerical profile features\n",
    "        \n",
    "        # LSTM Layer\n",
    "        h_embedding = self.embedding(x[0].long())\n",
    "        h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))\n",
    "        x[0], _  = self.lstm(h_embedding)\n",
    "        \n",
    "        # MaxPooling, BatchNormalisation and Dropout Layer\n",
    "        x[0] = self.globalMaxPool(x[0])\n",
    "        x[0] = self.batchnorm(x[0])\n",
    "        x[0] = self.dropout(x[0])\n",
    "\n",
    "        # Ensure x[1] is tensor format as float\n",
    "        # 2 Dense Layers\n",
    "        x[1] = torch.tensor(x[1], dtype=torch.float)\n",
    "        x[1] = self.relu(self.linear_one(x[1]))\n",
    "        x[1] = self.relu(self.linear_two(x[1]))\n",
    "\n",
    "        # Layer 3a: Concatenate the outputs of x[0] and x[1]\n",
    "        # x[0] has the 3rd dimension removed as its in the form [??,??,1] (ensures both x[0] and x[1] are same shape)\n",
    "        x[0] = x[0][:, :, -1]\n",
    "        conc = torch.cat((x[0],x[1]), 1)\n",
    "\n",
    "        # Concatenated tensor passed through Dense Layer\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        # Dense Layer which uses sigmoid activation function to return value between 0 and 1.\n",
    "        out = self.out(conc)\n",
    "        outF = self.outFunc(out)\n",
    "        return outF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7ccc8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialise Model\n",
    "model = Two_Input_Net(X2_train.shape[1], X1_train.shape[1])\n",
    "\n",
    "# Convert profile features to Variable format\n",
    "X2_train = Variable(torch.Tensor(X2_train).float())\n",
    "X2_test = Variable(torch.Tensor(X2_test).float())\n",
    "\n",
    "# # Convert to Variable format\n",
    "y_train = np.array(y_train)\n",
    "y_train = Variable(torch.LongTensor(y_train))\n",
    "y_test = np.array(y_test)\n",
    "y_test = Variable(torch.LongTensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec6c3817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Embedding: 1-1                         (201,900)\n",
      "├─LSTM: 1-2                              542,400\n",
      "├─AdaptiveMaxPool1d: 1-3                 --\n",
      "├─BatchNorm1d: 1-4                       300\n",
      "├─Dropout2d: 1-5                         --\n",
      "├─Linear: 1-6                            940\n",
      "├─ReLU: 1-7                              --\n",
      "├─Linear: 1-8                            1,650\n",
      "├─Linear: 1-9                            3,010\n",
      "├─Linear: 1-10                           11\n",
      "├─Sigmoid: 1-11                          --\n",
      "=================================================================\n",
      "Total params: 750,211\n",
      "Trainable params: 548,311\n",
      "Non-trainable params: 201,900\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Embedding: 1-1                         (201,900)\n",
       "├─LSTM: 1-2                              542,400\n",
       "├─AdaptiveMaxPool1d: 1-3                 --\n",
       "├─BatchNorm1d: 1-4                       300\n",
       "├─Dropout2d: 1-5                         --\n",
       "├─Linear: 1-6                            940\n",
       "├─ReLU: 1-7                              --\n",
       "├─Linear: 1-8                            1,650\n",
       "├─Linear: 1-9                            3,010\n",
       "├─Linear: 1-10                           11\n",
       "├─Sigmoid: 1-11                          --\n",
       "=================================================================\n",
       "Total params: 750,211\n",
       "Trainable params: 548,311\n",
       "Non-trainable params: 201,900\n",
       "================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed33954",
   "metadata": {},
   "source": [
    "# Creating Custom Dataset & Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea547e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,X1,X2,Y):\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "        self.Y = Y\n",
    "    def __len__(self):\n",
    "        return len(self.X1)\n",
    "    def __getitem__(self, idx):\n",
    "        text_data = self.X1[idx]\n",
    "        meta_data = self.X2[idx]\n",
    "        label = self.Y[idx]\n",
    "        return text_data, meta_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "495b719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Custom Dataset for both training and validation set\n",
    "training_dataset = CustomDataset(X1_train, X2_train, y_train)\n",
    "eval_dataset = CustomDataset(X1_test, X2_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a1517a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data loaders for both the training and validation set\n",
    "train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=128, shuffle=True)\n",
    "eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d0ffc2",
   "metadata": {},
   "source": [
    "# Main Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1b1405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    binaryConverted = torch.round(y_pred)\n",
    "    correct = (binaryConverted == y).sum() \n",
    "    return correct.float() / y.shape[0]\n",
    "\n",
    "def model_train(epoch):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    # MODEL: TRAIN\n",
    "    model.train()\n",
    "    \n",
    "    print(f\"---------------------Starting Epoch.. {(epoch+1)} ---------------------\")\n",
    "    progress_bar = tqdm(train_loader, desc='Processing Epoch {:1d}'.format((epoch+1)), leave=False, disable=False)\n",
    "    for text_data, meta_data, labels in progress_bar:\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the model output\n",
    "        yhat = model([text_data, meta_data])\n",
    "        # Calculate loss\n",
    "        labels = labels.unsqueeze(1)\n",
    "        loss = criterion(yhat, labels.float())\n",
    "        # Calculate accuracy\n",
    "        acc = calculate_accuracy(yhat, labels.float())\n",
    "        # Credit assignment\n",
    "        loss.backward()\n",
    "        # Update model weights\n",
    "        optimizer.step()\n",
    "        # Total training loss and accuracy\n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "        \n",
    "    return (train_loss/len(train_loader)), (train_acc/len(train_loader))\n",
    "\n",
    "def model_eval(epoch):\n",
    "        # MODEL: EVAL\n",
    "        model.eval()\n",
    "\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for text_data, meta_data, labels in eval_loader:\n",
    "                yhat = model([text_data, meta_data])\n",
    "                # calculate loss\n",
    "                labels = labels.unsqueeze(1)\n",
    "                loss = criterion(yhat, labels.float())\n",
    "\n",
    "                # Calculate accuracy\n",
    "                acc = calculate_accuracy(yhat, labels.float())\n",
    "                \n",
    "                # Total training loss and accuracy\n",
    "                eval_loss += loss.item()\n",
    "                eval_acc += acc.item()\n",
    "                                           \n",
    "        return (eval_loss/len(eval_loader)), (eval_acc/len(eval_loader))\n",
    "    \n",
    "def output(epoch, time, train_loss, train_acc, eval_loss, eval_acc):\n",
    "    print(f'Epoch: {(epoch+1)} | Epoch Duration (Seconds): {time}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.5f} | Train Accuracy: {train_acc:.5f}')\n",
    "    print(f'\\t Val. Loss: {eval_loss:.5f} |  Val. Accuracy: {eval_acc:.5f}')\n",
    "    print(f\"-------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89079e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Starting Epoch.. 1 ---------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Epoch 1:   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Epoch Duration (Seconds): 261.3110659122467\n",
      "\tTrain Loss: 0.18284 | Train Accuracy: 0.92526\n",
      "\t Val. Loss: 0.13081 |  Val. Accuracy: 0.95422\n",
      "-------------------------------------------------------------\n",
      "Updated Best Saved Model With Accuracy 0.954221299978403\n",
      "---------------------Starting Epoch.. 2 ---------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Epoch 2:   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Epoch Duration (Seconds): 252.4938929080963\n",
      "\tTrain Loss: 0.11738 | Train Accuracy: 0.95693\n",
      "\t Val. Loss: 0.10875 |  Val. Accuracy: 0.96164\n",
      "-------------------------------------------------------------\n",
      "Updated Best Saved Model With Accuracy 0.9616380150501544\n",
      "---------------------Starting Epoch.. 3 ---------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Epoch 3:   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Epoch Duration (Seconds): 253.65692019462585\n",
      "\tTrain Loss: 0.09230 | Train Accuracy: 0.96599\n",
      "\t Val. Loss: 0.08776 |  Val. Accuracy: 0.96605\n",
      "-------------------------------------------------------------\n",
      "Updated Best Saved Model With Accuracy 0.9660499224295983\n",
      "---------------------Starting Epoch.. 4 ---------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Epoch 4:   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Epoch Duration (Seconds): 248.55972504615784\n",
      "\tTrain Loss: 0.07804 | Train Accuracy: 0.97200\n",
      "\t Val. Loss: 0.07070 |  Val. Accuracy: 0.97325\n",
      "-------------------------------------------------------------\n",
      "Updated Best Saved Model With Accuracy 0.9732468916819645\n",
      "---------------------Starting Epoch.. 5 ---------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Epoch 5:   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Epoch Duration (Seconds): 249.85890197753906\n",
      "\tTrain Loss: 0.06496 | Train Accuracy: 0.97503\n",
      "\t Val. Loss: 0.07067 |  Val. Accuracy: 0.97285\n",
      "-------------------------------------------------------------\n",
      "---------------------Starting Epoch.. 6 ---------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Epoch 6:   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Epoch Duration (Seconds): 238.0498423576355\n",
      "\tTrain Loss: 0.06847 | Train Accuracy: 0.97411\n",
      "\t Val. Loss: 0.09833 |  Val. Accuracy: 0.96349\n",
      "-------------------------------------------------------------\n",
      "---------------------Starting Epoch.. 7 ---------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Epoch 7:   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Epoch Duration (Seconds): 238.15432691574097\n",
      "\tTrain Loss: 0.06590 | Train Accuracy: 0.97545\n",
      "\t Val. Loss: 0.06968 |  Val. Accuracy: 0.97378\n",
      "-------------------------------------------------------------\n",
      "Updated Best Saved Model With Accuracy 0.9737774381270775\n",
      "---------------------Starting Epoch.. 8 ---------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Epoch 8:   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Epoch Duration (Seconds): 232.66365694999695\n",
      "\tTrain Loss: 0.05470 | Train Accuracy: 0.97888\n",
      "\t Val. Loss: 0.06490 |  Val. Accuracy: 0.97550\n",
      "-------------------------------------------------------------\n",
      "Updated Best Saved Model With Accuracy 0.9754953384399414\n"
     ]
    }
   ],
   "source": [
    "TOTAL_EPOCH = 8\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "best_accuracy = -float('inf')\n",
    "\n",
    "for epoch in range(TOTAL_EPOCH):\n",
    "    # Model TRAIN\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = model_train(epoch)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Model Eval\n",
    "    eval_loss, eval_acc = model_eval(epoch)\n",
    "    \n",
    "    # Model output details\n",
    "    output(epoch, (end_time-start_time), train_loss, train_acc, eval_loss, eval_acc)\n",
    "    \n",
    "    \n",
    "    # Save Model with best accuracy\n",
    "    if (eval_acc > best_accuracy):\n",
    "        print(\"Updated Best Saved Model With Accuracy\", eval_acc)\n",
    "        best_accuracy = eval_acc\n",
    "        torch.save(model, 'two_input_NN_best_model.pt')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
